{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks and Time: Latent Space Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting node2vec\n",
      "  Downloading node2vec-0.4.6-py3-none-any.whl.metadata (743 bytes)\n",
      "Requirement already satisfied: gensim<5.0.0,>=4.1.2 in /opt/anaconda3/lib/python3.11/site-packages (from node2vec) (4.3.0)\n",
      "Requirement already satisfied: joblib<2.0.0,>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from node2vec) (1.2.0)\n",
      "Collecting networkx<3.0,>=2.5 (from node2vec)\n",
      "  Downloading networkx-2.8.8-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.19.5 in /opt/anaconda3/lib/python3.11/site-packages (from node2vec) (1.26.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.55.1 in /opt/anaconda3/lib/python3.11/site-packages (from node2vec) (4.65.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from gensim<5.0.0,>=4.1.2->node2vec) (1.11.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.11/site-packages (from gensim<5.0.0,>=4.1.2->node2vec) (5.2.1)\n",
      "Collecting FuzzyTM>=0.4.0 (from gensim<5.0.0,>=4.1.2->node2vec)\n",
      "  Downloading FuzzyTM-2.0.6-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (from FuzzyTM>=0.4.0->gensim<5.0.0,>=4.1.2->node2vec) (2.1.4)\n",
      "Collecting pyfume (from FuzzyTM>=0.4.0->gensim<5.0.0,>=4.1.2->node2vec)\n",
      "  Downloading pyFUME-0.3.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->FuzzyTM>=0.4.0->gensim<5.0.0,>=4.1.2->node2vec) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->FuzzyTM>=0.4.0->gensim<5.0.0,>=4.1.2->node2vec) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->FuzzyTM>=0.4.0->gensim<5.0.0,>=4.1.2->node2vec) (2023.3)\n",
      "Collecting simpful (from pyfume->FuzzyTM>=0.4.0->gensim<5.0.0,>=4.1.2->node2vec)\n",
      "  Downloading simpful-2.12.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting fst-pso (from pyfume->FuzzyTM>=0.4.0->gensim<5.0.0,>=4.1.2->node2vec)\n",
      "  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.11/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim<5.0.0,>=4.1.2->node2vec) (4.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->FuzzyTM>=0.4.0->gensim<5.0.0,>=4.1.2->node2vec) (1.16.0)\n",
      "Collecting miniful (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim<5.0.0,>=4.1.2->node2vec)\n",
      "  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hDownloading node2vec-0.4.6-py3-none-any.whl (7.0 kB)\n",
      "Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading FuzzyTM-2.0.6-py3-none-any.whl (29 kB)\n",
      "Downloading pyFUME-0.3.1-py3-none-any.whl (59 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading simpful-2.12.0-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: fst-pso, miniful\n",
      "  Building wheel for fst-pso (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20430 sha256=75a7c162a124dc506e295cb693d0f2a68cabccd4cc96dc2853b32af843a7fc3f\n",
      "  Stored in directory: /Users/teddyyankov/Library/Caches/pip/wheels/69/f5/e5/18ad53fe1ed6b2af9fad05ec052e4acbac8e92441df44bad2e\n",
      "  Building wheel for miniful (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3507 sha256=8856c5bb18672567e6533d54911e909d0edf79f0bb3673e32c13610166b270c6\n",
      "  Stored in directory: /Users/teddyyankov/Library/Caches/pip/wheels/9d/ff/2f/afe4cd56f47de147407705626517d68bea0f3b74eb1fb168e6\n",
      "Successfully built fst-pso miniful\n",
      "Installing collected packages: networkx, simpful, miniful, fst-pso, pyfume, FuzzyTM, node2vec\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.1\n",
      "    Uninstalling networkx-3.1:\n",
      "      Successfully uninstalled networkx-3.1\n",
      "Successfully installed FuzzyTM-2.0.6 fst-pso-1.8.1 miniful-0.0.6 networkx-2.8.8 node2vec-0.4.6 pyfume-0.3.1 simpful-2.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import zipfile\n",
    "import json\n",
    "import pickle\n",
    "import pycountry\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from scipy.sparse.csgraph import laplacian\n",
    "from scipy.linalg import eigh\n",
    "from scipy.integrate import quad\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trade "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading trade data in\n",
    "zf = zipfile.ZipFile('/Users/teddyyankov/Library/CloudStorage/OneDrive-Nexus365/Data-Driven Network Science/conference/trade_network_data.csv.zip')\n",
    "trade_df = pd.read_csv (zf.open('trade_network_data.csv'), index_col=0)\n",
    "trade_df.drop(trade_df.columns[trade_df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert country name to ISO3 code\n",
    "def get_iso3_code(country_name):\n",
    "    try:\n",
    "        country = pycountry.countries.get(name=country_name)\n",
    "        return country.alpha_3\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply the function to convert location_name_1 to ISO3 code\n",
    "trade_df['location_name_1'] = trade_df['location_name_1'].apply(get_iso3_code)\n",
    "\n",
    "# Apply the function to convert location_name_2 to ISO3 code\n",
    "trade_df['location_name_2'] = trade_df['location_name_2'].apply(get_iso3_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_df = trade_df.dropna(subset=['location_name_1', 'location_name_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply min-max normalization to the \"edge_value\" variable\n",
    "trade_df['edge_value'] = scaler.fit_transform(trade_df[['edge_value']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating graphs\n",
    "trade_graphs = []\n",
    "\n",
    "for i in sorted(trade_df['year'].unique()):\n",
    "\n",
    "    edges = pd.DataFrame(\n",
    "        {\n",
    "            \"source\": list(trade_df['location_name_1']),\n",
    "            \"target\": list(trade_df['location_name_2']),\n",
    "            \"weight\": list(trade_df['edge_value'])\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    G = nx.from_pandas_edgelist (edges, edge_attr = True)\n",
    "    G.graph['year'] = int(i)\n",
    "\n",
    "    # target_node = 'PAK'\n",
    "\n",
    "    # # Get the one-hop neighbors of the target node\n",
    "    # one_hop_neighbors = list(G.neighbors(target_node))\n",
    "    # one_hop_neighbors.append(target_node)  # Adding the target node itself\n",
    "\n",
    "    # # Create a subgraph with only the specified node and its one-hop neighbors\n",
    "    # subgraph = G.subgraph(one_hop_neighbors)\n",
    "\n",
    "    trade_graphs.append(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drawing an example network\n",
    "G = trade_graphs[2]\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "nx.draw_networkx (G, pos, with_labels=True, node_size=5, node_color='lightblue', font_size=7, font_color='black', edge_color='gray', width=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_trade_graphs = []\n",
    "\n",
    "# for graph in trade_graphs:\n",
    "    \n",
    "#     star_graph = nx.Graph()\n",
    "#     star_graph.add_nodes_from (graph.nodes())\n",
    "    \n",
    "#     for neighbor in graph.neighbors(\"PAK\"):\n",
    "#         star_graph.add_edge(\"PAK\", neighbor)\n",
    "    \n",
    "#     filtered_trade_graphs.append(star_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## drawing an example network\n",
    "# G = filtered_trade_graphs[2]\n",
    "# pos = nx.spring_layout(G, seed=42)\n",
    "# nx.draw_networkx (G, pos, with_labels=True, node_size=5, node_color='lightblue', font_size=7, font_color='black', edge_color='gray', width=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (filtered_trade_graphs[2].nodes() == trade_graphs[2].nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terrorism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ISO_pairs_terrorism2.csv file into a DataFrame\n",
    "iso_pairs_df = pd.read_csv(\"/Users/teddyyankov/Library/CloudStorage/OneDrive-Nexus365/Data-Driven Network Science/conference/ISO_pairs_terrorism3.csv\")\n",
    "\n",
    "# Filter the years in ISO_pairs_terrorism2 to contain the same years as trade_df\n",
    "filtered_iso_pairs_df = iso_pairs_df[iso_pairs_df['iyear'].isin(trade_df['year'])]\n",
    "filtered_iso_pairs_df = filtered_iso_pairs_df [(filtered_iso_pairs_df['source'] != \"IRN\") | (filtered_iso_pairs_df['target'] != \"IRN\")]\n",
    "# filtered_iso_pairs_df = filtered_iso_pairs_df [(filtered_iso_pairs_df['source'] == \"PAK\") | (filtered_iso_pairs_df['target'] == \"PAK\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_iso_pairs_df.dropna(subset=['source'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_iso_pairs_df['count'] = scaler.fit_transform(filtered_iso_pairs_df[['count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty graph\n",
    "terrorism_graphs = []\n",
    "\n",
    "# Iterate over each year in trade_df\n",
    "for i in sorted(trade_df['year'].unique()):\n",
    "    \n",
    "    # Empty graph\n",
    "    graph = nx.Graph()\n",
    "    \n",
    "    # index\n",
    "    index = int(i)\n",
    "    index = i - 2000\n",
    "    index = int (index)\n",
    "    \n",
    "    # Get the graph corresponding to the year from trade_graphs\n",
    "    year_graph = trade_graphs[index]\n",
    "    \n",
    "    # Add all the nodes from the year_graph to the graph\n",
    "    graph.add_nodes_from (year_graph.nodes())\n",
    "    \n",
    "    # Get the pairs of nodes for the current year from filtered_iso_pairs_df\n",
    "    year_pairs = filtered_iso_pairs_df [filtered_iso_pairs_df['iyear'] == i][['source', 'target', 'count']]\n",
    "    \n",
    "    # Add edges to the graph based on the pairs of nodes in year_pairs\n",
    "    for _, row in year_pairs.iterrows():\n",
    "        source = row['source']\n",
    "        target = row['target']\n",
    "        weight = row['count']\n",
    "        if source in graph.nodes() and target in graph.nodes():\n",
    "            graph.add_edge(source, target, weight = weight)\n",
    "    \n",
    "    # Remove nodes from graph that are not in year_graph\n",
    "    # graph.remove_nodes_from(node for node in graph.nodes() if node not in year_graph.nodes())\n",
    "    \n",
    "    # adding graph \n",
    "    terrorism_graphs.append(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = terrorism_graphs[14]\n",
    "pos = nx.spring_layout(G, seed=41, k = .8)\n",
    "nx.draw_networkx (G, pos, with_labels=True, node_size=5, node_color='lightblue', font_size=7, font_color='black', edge_color='gray', width=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (trade_graphs[14].nodes() == terrorism_graphs[14].nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mig_df = pd.read_csv (\"/Users/teddyyankov/Library/CloudStorage/OneDrive-Nexus365/Applied Analytical Statitics/Summative/dat_imputed.csv\")\n",
    "df_subset = pd.DataFrame (mig_df[['orig', 'dest', 'year', 'forced_mig']])\n",
    "df_subset_filtered = df_subset[df_subset['year'].isin(trade_df['year'])]\n",
    "df_subset_filtered['forced_mig'] = scaler.fit_transform(df_subset_filtered[['forced_mig']])\n",
    "display (df_subset_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty graph\n",
    "mig_graphs = []\n",
    "\n",
    "# Iterate over each year in trade_df\n",
    "for i in sorted(trade_df['year'].unique()):\n",
    "    \n",
    "    # Empty graph\n",
    "    graph = nx.Graph()\n",
    "    \n",
    "    # index\n",
    "    index = int(i)\n",
    "    index = i - 2000\n",
    "    index = int (index)\n",
    "    \n",
    "    # Get the graph corresponding to the year from trade_graphs\n",
    "    year_graph = trade_graphs[index]\n",
    "    \n",
    "    # Add all the nodes from the year_graph to the graph\n",
    "    graph.add_nodes_from (year_graph.nodes())\n",
    "    \n",
    "    # Get the pairs of nodes for the current year from filtered_iso_pairs_df\n",
    "    year_pairs = df_subset_filtered [df_subset_filtered['year'] == i][['orig', 'dest', 'forced_mig']]\n",
    "    \n",
    "    # Add edges to the graph based on the pairs of nodes in year_pairs\n",
    "    for _, row in year_pairs.iterrows():\n",
    "        source = row['orig']\n",
    "        target = row['dest']\n",
    "        weight = row['forced_mig']\n",
    "        if source in graph.nodes() and target in graph.nodes():\n",
    "            graph.add_edge(source, target, weight=weight)\n",
    "\n",
    "    # Remove nodes from graph that are not in year_graph\n",
    "    # graph.remove_nodes_from(node for node in graph.nodes() if node not in year_graph.nodes())\n",
    "\n",
    "    # adding graph \n",
    "    mig_graphs.append(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = mig_graphs[14]\n",
    "pos = nx.spring_layout(G, seed=41, k = .8)\n",
    "nx.draw_networkx (G, pos, with_labels=True, node_size=5, node_color='lightblue', font_size=7, font_color='black', edge_color='gray', width=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (mig_graphs[14].nodes() == terrorism_graphs[14].nodes())\n",
    "print (mig_graphs[14].nodes() == trade_graphs[14].nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save mig_graphs\n",
    "with open('mig_graphs.pkl', 'wb') as f:\n",
    "    pickle.dump(mig_graphs, f)\n",
    "\n",
    "# Save trade_graphs\n",
    "with open('trade_graphs.pkl', 'wb') as f:\n",
    "    pickle.dump(trade_graphs, f)\n",
    "\n",
    "# Save terrorism_graphs\n",
    "with open('terrorism_graphs.pkl', 'wb') as f:\n",
    "    pickle.dump(terrorism_graphs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spanning_tree_similarity(G1, G2):\n",
    "    '''\n",
    "    ## Parameters\n",
    "    G1: Graph object at time t\n",
    "    G2: Graph object at time t+n\n",
    "\n",
    "    ## Return \n",
    "    Spanning tree similarity metric\n",
    "    '''\n",
    "\n",
    "    # create empty list of eigenvalues \n",
    "    eigens = []\n",
    "\n",
    "    for G in [G1, G2]:\n",
    "        L = nx.normalized_laplacian_matrix(G) # generate the Laplacian\n",
    "\n",
    "        L_eigens = np.linalg.eigvals(L.toarray()) # obtain the eigenvalues of L\n",
    "\n",
    "        L_eigens = L_eigens[L_eigens>0] # get eigenvalues that are larger than 0 \n",
    "\n",
    "        eigens.append(sorted(L_eigens, reverse=False)) # sort from small to big \n",
    "\n",
    "    # calculate the product of eigen values\n",
    "        \n",
    "    ST_vals = []\n",
    "    \n",
    "    for eigen in eigens:\n",
    "        prod = np.prod(eigen)\n",
    "\n",
    "        ST = prod / len(eigen)\n",
    "\n",
    "        ST_vals.append(ST)\n",
    "    \n",
    "    # calculate the difference between the two values \n",
    "    distance = np.abs(np.log(ST_vals[1]) - np.log(ST_vals[0]))\n",
    "\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## distance functions \n",
    "## creating weighted versions of my functions\n",
    "# Jaccard\n",
    "def weighted_jaccard_distance(G1, G2):\n",
    "    \n",
    "    '''\n",
    "    Function to compute the weighted version of the Jaccard distance between two graphs\n",
    "    using their adjacency matrices\n",
    "    \n",
    "    - Input: a pair of network graph objects\n",
    "    - Output: Jaccard distance coefficient\n",
    "    '''\n",
    "    \n",
    "    # getting adjacency matrices of the graphs\n",
    "    A1 = nx.linalg.graphmatrix.adjacency_matrix (G1, weight='weight').todense()\n",
    "    A2 = nx.linalg.graphmatrix.adjacency_matrix (G2, weight='weight').todense()\n",
    "\n",
    "    # computing numerator and denominator of the weighted Jaccard distance\n",
    "    numerator = np.sum (np.abs (A1 - A2))\n",
    "    denominator = np.sum (np.maximum (A1, A2))\n",
    "\n",
    "    # computing the weighted Jaccard distance\n",
    "    jaccard_distance = numerator / denominator\n",
    "    return jaccard_distance\n",
    "\n",
    "# IM\n",
    "def weighted_ipsenMikhailov_distance (G1, G2, gamma = 0.1, limit = 100): \n",
    "    \n",
    "    '''\n",
    "    ADD DESCRIPTION\n",
    "    '''\n",
    "    from scipy.integrate import simps\n",
    "    \n",
    "    # weighted adjacency matrices\n",
    "    A1 = nx.linalg.graphmatrix.adjacency_matrix (G1, weight = 'weight').todense()\n",
    "    A2 = nx.linalg.graphmatrix.adjacency_matrix (G2, weight = 'weight').todense()\n",
    "    \n",
    "    # baseline function for IM distances\n",
    "    def IMdistance (A1, A2, gamma): \n",
    "            \n",
    "        # number of nodes \n",
    "        n = len(A1)\n",
    "            \n",
    "        # Laplacians\n",
    "        L1 = laplacian (A1, normed = False)\n",
    "        L2 = laplacian (A2, normed = False)\n",
    "            \n",
    "        # ω: vibrational frequencies\n",
    "        w1 = np.sqrt (np.abs (eigh (L1)[0][1:]))\n",
    "        w2 = np.sqrt (np.abs (eigh (L2)[0][1:]))\n",
    "            \n",
    "        # normalisation constants K (l2 norm)\n",
    "        norm1 = (n - 1) * np.pi / 2 - np.sum (np.arctan (-w1 / gamma))\n",
    "        norm2 = (n - 1) * np.pi / 2 - np.sum (np.arctan (-w2 / gamma))\n",
    "            \n",
    "        # spectral densitites ρ(ω, γ)\n",
    "        density1 = lambda w: np.sum (gamma / ((w - w1) ** 2 + gamma**2)) / norm1\n",
    "        density2 = lambda w: np.sum (gamma / ((w - w2) ** 2 + gamma**2)) / norm2\n",
    "            \n",
    "        # IM distance\n",
    "        func = lambda w: (density1(w) - density2(w)) ** 2\n",
    "        return np.sqrt (quad (func, 0, np.inf, limit = limit)[0])\n",
    "\n",
    "    # computing distance\n",
    "    distance = IMdistance (A1, A2, gamma)\n",
    "    return distance\n",
    "\n",
    "# poly \n",
    "def weighted_polynomial_distance (G1, G2, k = 5, alpha = 1): \n",
    "    \n",
    "    '''\n",
    "    Function to compute the polynomial spectral distance between two graphs\n",
    "    using their polynomial transformation of the eigenvalues of the\n",
    "    of the adjacency matrix in combination with the eigenvectors of the\n",
    "    adjacency matrix.\n",
    "    \n",
    "    - Input(s): \n",
    "            G1, G2 -> a pair of network graph objects\n",
    "            k -> maximum degree of the polynomial used in \n",
    "                 the polynomial dissimilarity distance calculation\n",
    "            alpha -> parameter controlling the influence of the \n",
    "                 polynomial transformation on the similarity score calculation\n",
    "    - Output: Polynomial distance coefficient\n",
    "    '''\n",
    "    \n",
    "    # getting adjacency matrices of the graphs\n",
    "    A1 = nx.linalg.graphmatrix.adjacency_matrix (G1, weight = 'weight').todense()\n",
    "    A2 = nx.linalg.graphmatrix.adjacency_matrix (G2, weight = 'weight').todense()\n",
    "    \n",
    "    # similarity \n",
    "    def similarity(A, k, alpha): \n",
    "        \n",
    "        # eigen-decomposition\n",
    "        eigVals, eigVec = np.linalg.eig(A)\n",
    "        \n",
    "        # shape of adjMatrix -> number of nodes\n",
    "        n = np.shape(A)[0]\n",
    "        \n",
    "        # defining polynomial\n",
    "        def polynomial(degree):\n",
    "            \n",
    "            # replicating formula\n",
    "            return eigVals**degree / (n - 1) ** (alpha * (degree - 1))\n",
    "        \n",
    "        # diagonal matrix constructed from the sum of the polynomial transformations\n",
    "        W = np.diag (sum([polynomial(k) for k in range (1, k + 1)]))\n",
    "        \n",
    "        # similarity score matrix \n",
    "        similarityScore = np.dot (np.dot (eigVec, W), eigVec.T)\n",
    "        return similarityScore\n",
    "    \n",
    "    # computing similarityScore for each adjMatrix\n",
    "    simi_A1 = similarity(A1, k, alpha)\n",
    "    simi_A2 = similarity(A2, k, alpha)\n",
    "    \n",
    "    # polynomial distance\n",
    "    polyDist = np.linalg.norm (simi_A1 - simi_A2, ord = \"fro\") / A1.shape[0] ** 2\n",
    "    \n",
    "    return polyDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Jaccard mig and terror series\n",
    "mig_ter_jaccard = []\n",
    "\n",
    "for i in range(len(mig_graphs)):\n",
    "    mig_graph = mig_graphs[i]\n",
    "    terrorism_graph = terrorism_graphs[i]\n",
    "    distance = weighted_jaccard_distance (mig_graph, terrorism_graph)\n",
    "    mig_ter_jaccard.append(distance)\n",
    "\n",
    "mig_ter_jaccard = pd.Series (mig_ter_jaccard)\n",
    "print (mig_ter_jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Jaccard trade and terror series\n",
    "tra_ter_jaccard = []\n",
    "\n",
    "for i in range(len(trade_graphs)):\n",
    "    trade_graph = trade_graphs[i]\n",
    "    terrorism_graph = terrorism_graphs[i]\n",
    "    distance = weighted_jaccard_distance (trade_graph, terrorism_graph)\n",
    "    tra_ter_jaccard.append(distance)\n",
    "\n",
    "tra_ter_jaccard = pd.Series (tra_ter_jaccard)\n",
    "print (tra_ter_jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Jaccard mig and terror series\n",
    "mig_ter_poly = []\n",
    "\n",
    "for i in range(len(mig_graphs)):\n",
    "    mig_graph = mig_graphs[i]\n",
    "    terrorism_graph = terrorism_graphs[i]\n",
    "    distance = weighted_polynomial_distance (mig_graph, terrorism_graph)\n",
    "    mig_ter_poly.append(distance)\n",
    "\n",
    "mig_ter_poly = pd.Series (mig_ter_poly)\n",
    "print (mig_ter_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Jaccard mig and terror series\n",
    "tra_ter_poly = []\n",
    "\n",
    "for i in range(len(trade_graphs)):\n",
    "    trade_graph = trade_graphs[i]\n",
    "    terrorism_graph = terrorism_graphs[i]\n",
    "    distance = weighted_polynomial_distance (trade_graph, terrorism_graph, k = 5, alpha = 1)\n",
    "    tra_ter_poly.append(distance)\n",
    "\n",
    "tra_ter_poly = pd.Series (tra_ter_poly)\n",
    "print (tra_ter_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spanning mig and terror\n",
    "mig_ter_span = []\n",
    "\n",
    "for i in range(len(mig_graphs)):\n",
    "    mig_graph = mig_graphs[i]\n",
    "    terrorism_graph = terrorism_graphs[i]\n",
    "    distance = spanning_tree_similarity (mig_graph, terrorism_graph)\n",
    "    mig_ter_span.append(distance)\n",
    "\n",
    "mig_ter_span = pd.Series (mig_ter_span)\n",
    "print (mig_ter_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spanning mig and terror\n",
    "tra_ter_span = []\n",
    "\n",
    "for i in range(len(mig_graphs)):\n",
    "    trade_graph = trade_graphs[i]\n",
    "    terrorism_graph = terrorism_graphs[i]\n",
    "    distance = spanning_tree_similarity (trade_graph, terrorism_graph)\n",
    "    tra_ter_span.append(distance)\n",
    "\n",
    "tra_ter_span = pd.Series (tra_ter_span)\n",
    "print (tra_ter_span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating average networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove final element of mig_graphs\n",
    "mig_graphs2 = mig_graphs.copy()\n",
    "mig_graphs2.pop()\n",
    "mig_graphs2.pop()\n",
    "\n",
    "# Create a new list for the scaled graphs\n",
    "scaled_mig_graphs = []\n",
    "\n",
    "# Calculate the scaling factors\n",
    "num_graphs = len(mig_graphs2)\n",
    "scaling_factors = np.exp(np.linspace(-1, 0, num_graphs))\n",
    "\n",
    "# Iterate over each graph in mig_graphs\n",
    "for i, graph in enumerate(mig_graphs2):\n",
    "    \n",
    "    # Create a new graph\n",
    "    scaled_graph = nx.Graph()\n",
    "\n",
    "    # Iterate over each edge in the current graph\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        # Scale the weight and add the edge to the new graph\n",
    "        scaled_weight = data['weight'] * scaling_factors[i]\n",
    "        scaled_graph.add_edge(u, v, weight=scaled_weight)\n",
    "\n",
    "    # Add the new graph to the list of scaled graphs\n",
    "    scaled_mig_graphs.append(scaled_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new graph\n",
    "mig_graph_flat = nx.Graph()\n",
    "\n",
    "# Iterate over each graph in mig_graphs\n",
    "for graph in scaled_mig_graphs:\n",
    "\n",
    "    # Iterate over each edge in the current graph\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "\n",
    "        # If the edge already exists in the combined_graph, add the weight to the existing weight\n",
    "        if mig_graph_flat.has_edge(u, v):\n",
    "            mig_graph_flat[u][v]['weight'] += data['weight']\n",
    "\n",
    "        # If the edge does not exist in the combined_graph, add it with the current weight\n",
    "        else:\n",
    "            mig_graph_flat.add_edge(u, v, weight=data['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mig_graph_flat.get_edge_data('SYR', 'GRC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove final element of mig_graphs\n",
    "trade_graphs2 = trade_graphs.copy()\n",
    "trade_graphs2.pop()\n",
    "trade_graphs2.pop()\n",
    "\n",
    "# Create a new list for the scaled graphs\n",
    "scaled_trade_graphs = []\n",
    "\n",
    "# Calculate the scaling factors\n",
    "num_graphs = len(trade_graphs2)\n",
    "scaling_factors = np.exp(np.linspace(-1, 0, num_graphs))\n",
    "\n",
    "# Iterate over each graph in mig_graphs\n",
    "for i, graph in enumerate(trade_graphs2):\n",
    "    \n",
    "    # Create a new graph\n",
    "    scaled_graph = nx.Graph()\n",
    "\n",
    "    # Iterate over each edge in the current graph\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        # Scale the weight and add the edge to the new graph\n",
    "        scaled_weight = data['weight'] * scaling_factors[i]\n",
    "        scaled_graph.add_edge(u, v, weight=scaled_weight)\n",
    "\n",
    "    # Add the new graph to the list of scaled graphs\n",
    "    scaled_trade_graphs.append(scaled_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new graph\n",
    "trade_graph_flat = nx.Graph()\n",
    "\n",
    "# Iterate over each graph in mig_graphs\n",
    "for graph in scaled_trade_graphs:\n",
    "\n",
    "    # Iterate over each edge in the current graph\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "\n",
    "        # If the edge already exists in the combined_graph, add the weight to the existing weight\n",
    "        if trade_graph_flat.has_edge(u, v):\n",
    "            trade_graph_flat[u][v]['weight'] += data['weight']\n",
    "\n",
    "        # If the edge does not exist in the combined_graph, add it with the current weight\n",
    "        else:\n",
    "            trade_graph_flat.add_edge(u, v, weight=data['weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terrorism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting the last graph in terrorism graphs\n",
    "terrorism_graphs2 = terrorism_graphs.copy()\n",
    "terrorism_graph_2020 = terrorism_graphs2[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mig-trade average network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new graph\n",
    "combined_graph_flat = nx.Graph()\n",
    "\n",
    "# Iterate over each edge in mig_graph_flat\n",
    "for u, v, data in mig_graph_flat.edges(data=True):\n",
    "\n",
    "    # If the edge also exists in trade_graph_flat, add the average weight to the combined_graph\n",
    "    if trade_graph_flat.has_edge(u, v):\n",
    "        avg_weight = (data['weight'] + trade_graph_flat[u][v]['weight']) / 2\n",
    "        combined_graph_flat.add_edge(u, v, weight=avg_weight)\n",
    "\n",
    "# Iterate over each edge in trade_graph_flat\n",
    "for u, v, data in trade_graph_flat.edges(data=True):\n",
    "    \n",
    "    # If the edge does not exist in the combined_graph (and therefore does not exist in mig_graph_flat), add it with the current weight\n",
    "    if not combined_graph_flat.has_edge(u, v):\n",
    "        combined_graph_flat.add_edge(u, v, weight=data['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the nodes from both graphs\n",
    "mig_nodes = set(mig_graph_flat.nodes)\n",
    "trade_nodes = set(trade_graph_flat.nodes)\n",
    "\n",
    "# Find the common nodes\n",
    "common_nodes = mig_nodes.intersection(trade_nodes)\n",
    "\n",
    "print(common_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing LSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## libraries\n",
    "from node2vec import Node2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import random\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all nodes in the graph\n",
    "nodes = list(terrorism_graph_2020.nodes)\n",
    "\n",
    "# Generate non-edges\n",
    "non_edges = []\n",
    "while len(non_edges) < len(terrorism_graph_2020.edges):  # generate the same number of non-edges as there are edges\n",
    "    # Randomly select two nodes\n",
    "    i, j = random.sample(nodes, 2)\n",
    "\n",
    "    # If there is no edge between them, add them as a non-edge\n",
    "    if not terrorism_graph_2020.has_edge(i, j):\n",
    "        non_edges.append((i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d75c94574df4de4970444c576ae2a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/211 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 50/50 [00:04<00:00, 11.92it/s]\n",
      "Generating walks (CPU: 2): 100%|██████████| 50/50 [00:04<00:00, 11.85it/s]\n",
      "Generating walks (CPU: 3): 100%|██████████| 50/50 [00:04<00:00, 12.06it/s]\n",
      "Generating walks (CPU: 4): 100%|██████████| 50/50 [00:04<00:00, 11.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Generate Node2Vec embeddings\n",
    "node2vec = Node2Vec(combined_graph_flat, dimensions=64, walk_length=30, num_walks=200, workers=4)\n",
    "model = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# Prepare the link prediction data\n",
    "positive_examples = [np.concatenate ((model.wv[str(i)], model.wv[str(j)])) for i, j in terrorism_graph_2020.edges]\n",
    "negative_examples = [np.concatenate ((model.wv[str(i)], model.wv[str(j)])) for i, j in non_edges] \n",
    "X = positive_examples + negative_examples\n",
    "y = [1] * len (positive_examples) + [0] * len (negative_examples)\n",
    "X_train, X_test, y_train, y_test = train_test_split (X, y, test_size = 0.5, random_state = 42)\n",
    "\n",
    "# Train a logistic regression model\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = clf.predict_proba(X_test)[:, 1]\n",
    "print('AUC-ROC:', roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e391a4bae7f74cdc8485473467d3b5bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/211 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 50/50 [00:04<00:00, 11.75it/s]\n",
      "Generating walks (CPU: 2): 100%|██████████| 50/50 [00:04<00:00, 11.11it/s]\n",
      "Generating walks (CPU: 3): 100%|██████████| 50/50 [00:04<00:00, 11.40it/s]\n",
      "Generating walks (CPU: 4): 100%|██████████| 50/50 [00:04<00:00, 11.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC for fold 1: 0.75\n",
      "AUC-ROC for fold 2: 1.0\n",
      "AUC-ROC for fold 3: 0.5\n",
      "AUC-ROC for fold 4: 1.0\n",
      "AUC-ROC for fold 5: 0.0\n",
      "AUC-ROC for fold 6: 1.0\n",
      "AUC-ROC for fold 7: 1.0\n",
      "AUC-ROC for fold 8: 1.0\n",
      "AUC-ROC for fold 9: 1.0\n",
      "AUC-ROC for fold 10: 1.0\n",
      "AUC-ROC for fold 11: 1.0\n",
      "AUC-ROC for fold 12: 1.0\n",
      "AUC-ROC for fold 13: 1.0\n",
      "AUC-ROC for fold 14: 1.0\n",
      "AUC-ROC for fold 15: 1.0\n",
      "Average AUC-ROC: 0.8833333333333333\n"
     ]
    }
   ],
   "source": [
    "# Generate Node2Vec embeddings\n",
    "node2vec = Node2Vec(combined_graph_flat, dimensions=64, walk_length=30, num_walks=200, workers=4)\n",
    "model = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# Prepare the link prediction data\n",
    "positive_examples = [np.concatenate((model.wv[str(i)], model.wv[str(j)])) for i, j in terrorism_graph_2020.edges]\n",
    "negative_examples = [np.concatenate((model.wv[str(i)], model.wv[str(j)])) for i, j in non_edges]\n",
    "X = positive_examples + negative_examples\n",
    "y = [1] * len(positive_examples) + [0] * len(negative_examples)\n",
    "\n",
    "# Train a logistic regression model with 15-fold cross-validation\n",
    "clf = LogisticRegression(random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=15, scoring='roc_auc')\n",
    "\n",
    "# Print the AUC-ROC score for each fold\n",
    "for i, score in enumerate(scores, start=1):\n",
    "    print(f'AUC-ROC for fold {i}: {score}')\n",
    "\n",
    "# Print the average AUC-ROC score\n",
    "print('Average AUC-ROC:', scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f29509da7fd94463a096e5bf10906439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/211 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 50/50 [00:04<00:00, 11.98it/s]\n",
      "Generating walks (CPU: 2): 100%|██████████| 50/50 [00:04<00:00, 12.21it/s]\n",
      "Generating walks (CPU: 3): 100%|██████████| 50/50 [00:04<00:00, 12.20it/s]\n",
      "Generating walks (CPU: 4): 100%|██████████| 50/50 [00:04<00:00, 12.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average AUC-ROC: 0.8666666666666667\n",
      "Average Accuracy: 0.6944444444444443\n",
      "Average F1 Score: 0.6888888888888887\n"
     ]
    }
   ],
   "source": [
    "# Generate Node2Vec embeddings\n",
    "node2vec = Node2Vec(combined_graph_flat, dimensions=64, walk_length=30, num_walks=200, workers=4)\n",
    "model = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# Prepare the link prediction data\n",
    "positive_examples = [np.concatenate((model.wv[str(i)], model.wv[str(j)])) for i, j in terrorism_graph_2020.edges]\n",
    "negative_examples = [np.concatenate((model.wv[str(i)], model.wv[str(j)])) for i, j in non_edges]\n",
    "X = positive_examples + negative_examples\n",
    "y = [1] * len(positive_examples) + [0] * len(negative_examples)\n",
    "\n",
    "# Train a logistic regression model with 15-fold cross-validation\n",
    "clf = LogisticRegression(random_state=0)\n",
    "\n",
    "# Calculate and print the AUC-ROC score for each fold\n",
    "scores_auc = cross_val_score(clf, X, y, cv=15, scoring='roc_auc')\n",
    "print('Average AUC-ROC:', scores_auc.mean())\n",
    "\n",
    "# Calculate and print the accuracy for each fold\n",
    "scores_accuracy = cross_val_score(clf, X, y, cv=15, scoring='accuracy')\n",
    "print('Average Accuracy:', scores_accuracy.mean())\n",
    "\n",
    "# Calculate and print the F1 score for each fold\n",
    "scores_f1 = cross_val_score(clf, X, y, cv=15, scoring='f1')\n",
    "print('Average F1 Score:', scores_f1.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
